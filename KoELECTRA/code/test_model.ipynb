{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "모델의 성능을 확인하기 위한 코드이다."
      ],
      "metadata": {
        "id": "LGcY-H3SiOph"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1. 구글 드라이브를 코랩에 연결한다.**\n",
        "> 이는 추후 모델을 불러오고, 학습할 데이터를 불러오기 위해 필요한 과정이다. \\\n",
        " 따라서 이 코드를 실행하기 전에, 구글 드라이브에 모델과 토크나이저, 전처리된 데이터를 업로드 해야 한다."
      ],
      "metadata": {
        "id": "PUOObFrraXrY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooRw3VlCY3rE"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2. 테스트를 위해 필요한 라이브러리를 불러온다.**"
      ],
      "metadata": {
        "id": "5PE-K3yt-egE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-xgz4aTYgVi"
      },
      "outputs": [],
      "source": [
        "!pip install transformers seqeval[gpu]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qhw9YMAiYgVn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import ElectraTokenizerFast, ElectraConfig, ElectraForTokenClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBTc5iX_YgVo"
      },
      "outputs": [],
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3. 테스트 데이터셋을 불러온다.**\n",
        "> ***데이터의 경로 입력/수정 필수!***"
      ],
      "metadata": {
        "id": "_IBAfq80-rco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/gdrive/MyDrive/2022_lesik_workspace/lesik/data/new_no_pool_test.tsv', sep = '\\t', keep_default_na=False)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "9mV0kRUP5kk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "태그는 학습된 모델과 동일해야 되므로 변경해서는 안된다."
      ],
      "metadata": {
        "id": "tth3XlA1ghtU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_CDSb4AZ0C5"
      },
      "outputs": [],
      "source": [
        "# Split labels based on whitespace and turn them into a list\n",
        "arr_labels = set()\n",
        "for lb in df.label:\n",
        "    lb = lb.split()\n",
        "    for ll in lb:\n",
        "        if ll not in arr_labels:\n",
        "            arr_labels.add(ll)\n",
        "\n",
        "#말뭉치 데이터에 포함된 총 태그\n",
        "unique_labels = {'OGG_EDUCATION', 'MT_ELEMENT', 'AFW_OTHER_PRODUCTS', 'MT_ROCK', 'TI_OTHERS', 'PS_NAME', 'CV_BUILDING_TYPE', 'AM_REPTILIA', 'OGG_FOOD', 'AF_MUSICAL_INSTRUMENT', 'AF_BUILDING', 'AFA_MUSIC', 'CV_SPORTS_INST', 'QT_ORDER', 'TM_COLOR', 'LCG_MOUNTAIN', 'QT_MAN_COUNT', 'PS_CHARACTER', 'AM_OTHERS', 'OGG_LIBRARY', 'TMM_DISEASE', 'OGG_MEDICINE', 'LCG_ISLAND', 'TI_MINUTE', 'MT_CHEMICAL', 'TM_CELL_TISSUE_ORGAN', 'QT_OTHERS', 'CV_TRIBE', 'QT_TEMPERATURE', 'PT_FLOWER', 'OGG_POLITICS', 'DT_WEEK', 'FD_ART', 'AM_AMPHIBIA', 'FD_MEDICINE', 'AF_CULTURAL_ASSET', 'AF_TRANSPORT', 'EV_SPORTS', 'LCG_CONTINENT', 'PT_TREE', 'TMI_SERVICE', 'AM_MAMMALIA', 'TM_SPORTS', 'CV_INGREDIENT', 'OGG_HOTEL', 'QT_PHONE', 'CV_LANGUAGE', 'CV_FUNDS', 'CV_CURRENCY', 'FD_OTHERS', 'LCG_RIVER', 'LCP_CAPITALCITY', 'LC_OTHERS', 'QT_SIZE', 'TM_CLIMATE', 'TM_SHAPE', 'CV_POLICY', 'EV_ACTIVITY', 'TR_ART', 'QT_ADDRESS', 'OGG_RELIGION', 'CV_POSITION', 'FD_HUMANITIES', 'CV_CULTURE', 'QT_SPORTS', 'QT_ALBUM', 'CV_ART', 'CV_FOOD', 'CV_LAW', 'OGG_MILITARY', 'DT_DAY', 'FD_SOCIAL_SCIENCE', 'LCP_PROVINCE', 'CV_CLOTHING', 'TI_HOUR', 'DT_DYNASTY', 'DT_SEASON', 'FD_SCIENCE', 'TMI_HW', 'OGG_SPORTS', 'TR_OTHERS', 'TM_DIRECTION', 'TMI_SITE', 'QT_LENGTH', 'MT_METAL', 'LCG_OCEAN', 'DT_OTHERS', 'LCP_COUNTY', 'TMIG_GENRE', 'OGG_ECONOMY', 'TMI_SW', 'CV_SPORTS_POSITION', 'AFA_DOCUMENT', 'PT_OTHERS', 'AFA_ART_CRAFT', 'EV_OTHERS', 'TMI_EMAIL', 'QT_PRICE', 'EV_FESTIVAL', 'TI_SECOND', 'CV_TAX', 'O', 'QT_VOLUME', 'AF_WEAPON', 'LCG_BAY', 'OGG_SCIENCE', 'PT_FRUIT', 'CV_OCCUPATION', 'QT_CHANNEL', 'OGG_ART', 'AM_INSECT', 'CV_FOOD_STYLE', 'QT_PERCENTAGE', 'OGG_LAW', 'TR_SCIENCE', 'CV_RELATION', 'AM_PART', 'QT_AGE', 'TMI_MODEL', 'AM_BIRD', 'OGG_OTHERS', 'CV_SPORTS', 'DT_YEAR', 'LCP_COUNTRY', 'AFA_VIDEO', 'DT_GEOAGE', 'TI_DURATION', 'AM_TYPE', 'CV_SEASONING', 'AM_FISH', 'CV_PRIZE', 'PS_PET', 'AFW_SERVICE_PRODUCTS', 'TMI_PROJECT', 'CV_DRINK', 'LC_SPACE', 'LCP_CITY', 'EV_WAR_REVOLUTION', 'AFA_PERFORMANCE', 'QT_SPEED', 'PT_GRASS', 'DT_MONTH', 'PT_PART', 'OGG_MEDIA', 'PT_TYPE', 'TMM_DRUG', 'AF_ROAD', 'DT_DURATION', 'TR_MEDICINE', 'TR_HUMANITIES'}\n",
        "print(unique_labels)\n",
        "\n",
        "# Map each label into its id representation and vice versa\n",
        "labels_to_ids = {k: v for v, k in enumerate(sorted(unique_labels))}\n",
        "ids_to_labels = {v: k for v, k in enumerate(sorted(unique_labels))}\n",
        "\n",
        "prob_tag_list=[]\n",
        "for lb in arr_labels:\n",
        "    if lb not in labels_to_ids:\n",
        "        prob_tag_list.append(\"'\"+lb+\"'\")\n",
        "print(\",\".join(prob_tag_list))\n",
        "\n",
        "#말뭉치에 포함되어 있지 않는 태그들 추가\n",
        "labels_to_ids['CV_ACT'] = 150\n",
        "ids_to_labels[150] = 'CV_ACT'\n",
        "\n",
        "labels_to_ids['CV_STATE'] = 151\n",
        "ids_to_labels[151] = 'CV_STATE'\n",
        "\n",
        "print(ids_to_labels)\n",
        "print(len(ids_to_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lx5HgHShLwS0"
      },
      "outputs": [],
      "source": [
        "# Let's take a look at how can we preprocess the text - Take first example\n",
        "text = df['text'].values.tolist()\n",
        "m_len = 0\n",
        "for t in text:\n",
        "    if m_len < len(t):\n",
        "        m_len = len(t)\n",
        "        \n",
        "example = text[1]\n",
        "\n",
        "print(example)\n",
        "print(m_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**4. 테스트를 진행 할 최종 모델과 토크나이저를 불러온다.**\n",
        "> ***모델, 토크나이저, epoch 입력/수정 필수!***\n",
        "\n"
      ],
      "metadata": {
        "id": "19orQlfVAOQP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-dEWXr-YgVs"
      },
      "outputs": [],
      "source": [
        "def load(epoch):\n",
        "    model_directory = '/content/gdrive/MyDrive/2022_lesik_workspace/lesik/model/FIXED_FINAL_EPOCH_'+ str(epoch) #모델 경로\n",
        "    model = ElectraForTokenClassification.from_pretrained(model_directory, num_labels=len(labels_to_ids))\n",
        "    model.to(device)\n",
        "    \n",
        "    tokenizer_directory = '/content/gdrive/MyDrive/2022_lesik_workspace/lesik/tokenizer/FIXED_FINAL_EPOCH_' +str(epoch) #토크나이저 경로\n",
        "    tokenizer = ElectraTokenizerFast.from_pretrained(tokenizer_directory)\n",
        "    \n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "argument로 불러오기를 원하는 epoch를 적는다."
      ],
      "metadata": {
        "id": "Lu06Q-3-Arba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 72   #원하는 epoch 변경\n",
        "model, tokenizer = load(epoch)"
      ],
      "metadata": {
        "id": "7_n7KIU0AyMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**5. 토큰화를 하기 위해 필요한 코드이다.**\n",
        "> ***원하는 epoch로 수정 가능!***"
      ],
      "metadata": {
        "id": "XnjGi75sAh0e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXi-qGnGYgVu"
      },
      "outputs": [],
      "source": [
        "from transformers import ElectraTokenizerFast\n",
        "\n",
        "MAX_LEN = 256\n",
        "TEST_BATCH_SIZE = 8\n",
        "EPOCH = 72\n",
        "LEARNING_RATE = 1e-05\n",
        "MAX_GRAD_NORM = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTaA59eJYgVv"
      },
      "outputs": [],
      "source": [
        "class ElectraDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # step 1: get the sentence and word labels \n",
        "        sentence = self.data.text[index].strip()\n",
        "        word_labels = self.data.label[index].split()\n",
        "\n",
        "        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n",
        "        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n",
        "        encoding = self.tokenizer(sentence,\n",
        "                             return_offsets_mapping=True, \n",
        "                             padding='max_length', \n",
        "                             truncation=True, \n",
        "                             max_length=self.max_len)\n",
        "        \n",
        "        valid_token_list = []\n",
        "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
        "            if mapping[0] == 0 and mapping[1] == 0:\n",
        "                continue\n",
        "            valid_token_list.append(mapping)\n",
        "        if len(valid_token_list) != len(word_labels):\n",
        "            print(index, len(word_labels), len(valid_token_list), sentence)\n",
        "        \n",
        "        # step 3: create token labels only for first word pieces of each tokenized word\n",
        "        labels = [labels_to_ids[label] for label in word_labels] \n",
        "        # code based on https://huggingface.co/transformers/custom_datasets.html#tok-ner\n",
        "        # create an empty array of -100 of length max_length\n",
        "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
        "        \n",
        "        # set only labels whose first offset position is 0 and the second is not 0\n",
        "        i = 0\n",
        "        if len(labels) != 0:\n",
        "            for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
        "                if mapping[0] == 0 and mapping[1] == 0:\n",
        "                    continue\n",
        "                tok = tokenizer.convert_ids_to_tokens(encoding['input_ids'][idx])\n",
        "            \n",
        "                # overwrite label\n",
        "                if i == len(labels):\n",
        "                    break\n",
        "                encoded_labels[idx] = labels[i]\n",
        "                i += 1\n",
        "                \n",
        "        # step 4: turn everything into PyTorch tensors\n",
        "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
        "        item['label'] = torch.as_tensor(encoded_labels)\n",
        "        \n",
        "        return item\n",
        "    def __len__(self):\n",
        "        return self.len"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testing_set = ElectraDataset(df, tokenizer, MAX_LEN)"
      ],
      "metadata": {
        "id": "C4NKG0d9BiDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Cv53qH97a8P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4fd5d82-33b6-4677-bae9-be2ebfe75627"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "test_params = {'batch_size': TEST_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 4\n",
        "                }\n",
        "\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODYsOGm_YgVx"
      },
      "outputs": [],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knuKPJOiHbiZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f66cd686-34b4-4409-8aaf-be0cb134fb38"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0693, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "inputs = testing_set[2]\n",
        "input_ids = inputs[\"input_ids\"].unsqueeze(0)\n",
        "attention_mask = inputs[\"attention_mask\"].unsqueeze(0)\n",
        "labels = inputs[\"label\"].unsqueeze(0)\n",
        "\n",
        "input_ids = input_ids.to(device)\n",
        "attention_mask = attention_mask.to(device)\n",
        "labels = labels.to(device)\n",
        "\n",
        "outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "initial_loss = outputs[0]\n",
        "initial_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URciATH5YgVy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45b2a3ac-1e21-4597-ec14-ed92c15bd0d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 256, 152])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "tr_logits = outputs[1]\n",
        "tr_logits.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSjVszsAYgVy"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**6. test 함수를 불러오는 섹션이다.**"
      ],
      "metadata": {
        "id": "BsQkPbB5B5EF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjRtfFFJYgVy"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "def test():\n",
        "    # put model in evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    test_loss, test_accuracy = 0, 0\n",
        "    nb_test_examples, nb_test_steps = 0, 0\n",
        "    test_preds, test_labels = [], []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(testing_loader):\n",
        "            \n",
        "            ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "            labels = batch['label'].to(device, dtype = torch.long)\n",
        "            \n",
        "            output = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
        "            loss = output[0]\n",
        "            test_logits = output[1]\n",
        "            \n",
        "            test_loss += loss.item()\n",
        "\n",
        "            nb_test_steps += 1\n",
        "            nb_test_examples += labels.size(0)\n",
        "        \n",
        "            if idx % 100==0:\n",
        "                loss_step = test_loss/nb_test_steps\n",
        "                print(f\"Test loss per 100 test steps: {loss_step}\")\n",
        "              \n",
        "            # compute evaluation accuracy\n",
        "            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = test_logits.view(-1, model.config.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "            \n",
        "            # only compute accuracy at active labels\n",
        "            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
        "        \n",
        "            labels = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "            \n",
        "            test_labels.extend(labels)\n",
        "            test_preds.extend(predictions)\n",
        "            \n",
        "            tmp_test_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
        "            test_accuracy += tmp_test_accuracy\n",
        "\n",
        "    labels = [ids_to_labels[id.item()] for id in test_labels]\n",
        "    predictions = [ids_to_labels[id.item()] for id in test_preds]\n",
        "    print(classification_report(labels, predictions))\n",
        "    test_loss = test_loss / nb_test_steps\n",
        "    test_accuracy = test_accuracy / nb_test_steps\n",
        "    print(f\"Test Loss: {test_loss}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**TEST 결과**"
      ],
      "metadata": {
        "id": "8LzfWnTkCCGN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJbOtD9fYnN1",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "test()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**(선택) predict 함수이다.**"
      ],
      "metadata": {
        "id": "D7YIn6zsbFtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#예측하고 싶은 텍스트를 넣어주세요.\n",
        "predict_text = '다진 마늘을 넣고 잘게 자른 김치를 섞어주세요.'"
      ],
      "metadata": {
        "id": "ajhENtTODgLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(sentence):\n",
        "    result = []\n",
        "    inputs = tokenizer(sentence,\n",
        "                        return_offsets_mapping=True, \n",
        "                        padding='max_length', \n",
        "                        truncation=True, \n",
        "                        max_length=MAX_LEN,\n",
        "                        return_tensors=\"pt\")\n",
        "\n",
        "    # move to gpu\n",
        "    ids = inputs[\"input_ids\"].to(device)\n",
        "    mask = inputs[\"attention_mask\"].to(device)\n",
        "    # forward pass\n",
        "    outputs = model(ids, attention_mask=mask)\n",
        "    logits = outputs[0]\n",
        "\n",
        "    active_logits = logits.view(-1, model.config.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "    flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n",
        "\n",
        "    tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
        "    token_predictions = [ids_to_labels[i] for i in flattened_predictions.cpu().numpy()]\n",
        "    wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n",
        "\n",
        "    word_prediction = []\n",
        "    token_prediction = []\n",
        "    pred_dict = {}\n",
        "    token_dict = {}\n",
        "    for token_pred, word_idx in zip(wp_preds, inputs.word_ids()):\n",
        "        if token_pred[0] not in ['[CLS]','[UNK]','[PAD]','[SEP]']:\n",
        "            result.append([word_idx, token_pred[0], token_pred[1]]) # \n",
        "            #print(token_pred)\n",
        "            if word_idx not in pred_dict:\n",
        "                pred_dict[word_idx] = set()\n",
        "            if word_idx not in token_dict:\n",
        "                token_dict[word_idx] = \"\"\n",
        "            pred_dict[word_idx].add(token_pred[1])\n",
        "            token_dict[word_idx] += token_pred[0].replace(\"#\", \"\")\n",
        "                \n",
        "    for token_pred, word_idx in zip(wp_preds, inputs.word_ids()):\n",
        "        #only predictions on first word pieces are important\n",
        "        if token_pred[0] not in ['[CLS]','[UNK]','[PAD]','[SEP]']:\n",
        "            if token_pred[1] != 'O':\n",
        "                token_prediction.append([word_idx, token_pred[0],token_pred[1]])\n",
        "    \n",
        "    #for i in range(len(token_prediction)):\n",
        "    #    print(token_prediction[i], word_idx)\n",
        "    #print()\n",
        "    return result"
      ],
      "metadata": {
        "id": "e7iahMTihUWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JfupT4vYgV0",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "predict(predict_text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "PyTorch 1.13 (NGC 22.05/Python 3.8 Conda) on Backend.AI",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}