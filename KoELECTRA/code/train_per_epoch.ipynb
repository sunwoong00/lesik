{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "모델을 가장 처음에 학습하기 위한 코드이다. 이후에 파인튜닝을 진행할 시 태그를 더 추가할 수 없기 때문에 필요한 태그들은 아래 추가한 후 진행해야 된다. 이 코드는 gpu 사용량 때문에 colab에서 실행하지 못했고 backend ai를 사용했다."
      ],
      "metadata": {
        "id": "q3EBR4IMioxs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1. 학습을 위해 필요한 라이브러리를 불러온다.**"
      ],
      "metadata": {
        "id": "Mi7vx64tzLDZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqEcaK7ky3o3"
      },
      "outputs": [],
      "source": [
        "!pip install transformers seqeval[gpu]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hCoWlTpy3pH",
        "outputId": "03aa728f-4e8b-4c1e-94e2-bc3f776c48ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4tOkol7y3pL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import ElectraTokenizerFast, ElectraConfig, ElectraForTokenClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2. 학습을 위한 데이터셋을 불러온다.**\n",
        "> ***데이터의 경로 입력/수정 필수!***"
      ],
      "metadata": {
        "id": "thexIUwfzXpT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBiNy3EMy3pO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/home/work/lesik/data/modified_labeled_fixed_dataset.tsv', sep = '\\t', keep_default_na=False)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   첫 학습 이후에 파인튜닝을 진행할 시 태그를 더 추가할 수 없기 때문에\n",
        "fine_tuning과 test를 진행할 때는 무조건 태그가 동일해야 된다.\n",
        "*   필요한 태그는 전부 아래 추가한 후 진행해야 된다."
      ],
      "metadata": {
        "id": "Lp5uo5ajf-ja"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_CDSb4AZ0C5"
      },
      "outputs": [],
      "source": [
        "# Split labels based on whitespace and turn them into a list\n",
        "labels = [i.split() for i in df['label'].values.tolist()]\n",
        "\n",
        "# Check how many labels are there in the dataset\n",
        "#말뭉치 데이터에 포함된 총 태그\n",
        "unique_labels = set()\n",
        "for lb in labels:\n",
        "  [unique_labels.add(i) for i in lb if i not in unique_labels]\n",
        "\n",
        "# Map each label into its id representation and vice versa\n",
        "labels_to_ids = {k: v for v, k in enumerate(sorted(unique_labels))}\n",
        "ids_to_labels = {v: k for v, k in enumerate(sorted(unique_labels))}\n",
        "\n",
        "#말뭉치에 포함되어 있지 않는 태그들 추가\n",
        "labels_to_ids['CV_ACT'] = 150\n",
        "ids_to_labels[150] = 'CV_ACT'\n",
        "\n",
        "labels_to_ids['CV_STATE'] = 151\n",
        "ids_to_labels[151] = 'CV_STATE'\n",
        "\n",
        "print(ids_to_labels)\n",
        "print(len(ids_to_labels))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's take a look at how can we preprocess the text - Take first example\n",
        "text = df['text'].values.tolist()\n",
        "m_len = 0\n",
        "for t in text:\n",
        "    if m_len < len(t):\n",
        "        m_len = len(t)\n",
        "        \n",
        "example = text[1]\n",
        "\n",
        "print(example)\n",
        "print(m_len)"
      ],
      "metadata": {
        "id": "1f9grbm3ABlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**3. koELECTRA 모델과 토크나이저를 불러온다.**"
      ],
      "metadata": {
        "id": "O9luEhCw4X8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ElectraForTokenClassification.from_pretrained(\"monologg/koelectra-small-v2-discriminator\", num_labels=len(labels_to_ids))  #koELECTRA 모델\n",
        "model.to(device)\n",
        "tokenizer = ElectraTokenizerFast.from_pretrained('monologg/koelectra-small-v2-discriminator') #koELECTRA 토크나이저"
      ],
      "metadata": {
        "id": "DCxaWqRp3mH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**4. 토큰화를 하기 위해 필요한 코드이다.**\n",
        "> ***원하는 epoch로 수정 가능!***"
      ],
      "metadata": {
        "id": "ssiYrVYFz7gN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "epoch 개수는 고정이 아니므로 각 모델에 적절 또는 최적화 되어있는 개수로 변경하면 된다. \\\n",
        "학습 중에 중단 되었을 경우, 저장된 epoch부터 이어서 학습 시킬 수 있다. 단, epochs를 저장된 epoch만큼 빼서 변경해줘야한다.\n",
        "> ex.) epoch 72를 목표하였고, epoch 48까지 저장된 후 중단 되었다면 \\\n",
        "72-48 = 24; epochs를 24로 변경해주면 된다. \n",
        "\n",
        "(7.학습 실행 코드 참고)"
      ],
      "metadata": {
        "id": "JaRuvh-f1FCd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E30tOyaRy3pe"
      },
      "outputs": [],
      "source": [
        "from transformers import ElectraTokenizerFast\n",
        "\n",
        "MAX_LEN = 256\n",
        "TRAIN_BATCH_SIZE = 64\n",
        "VALID_BATCH_SIZE = 64\n",
        "EPOCHS = 72\n",
        "LEARNING_RATE = 1e-05\n",
        "MAX_GRAD_NORM = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lNxqhTVy3pn"
      },
      "outputs": [],
      "source": [
        "class ElectraDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # step 1: get the sentence and word labels \n",
        "        sentence = self.data.text[index].strip()\n",
        "        word_labels = self.data.label[index].split()\n",
        "\n",
        "        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n",
        "        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n",
        "        encoding = self.tokenizer(sentence,\n",
        "                             return_offsets_mapping=True, \n",
        "                             padding='max_length', \n",
        "                             truncation=True, \n",
        "                             max_length=self.max_len)\n",
        "        \n",
        "        valid_token_list = []\n",
        "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
        "            if mapping[0] == 0 and mapping[1] == 0:\n",
        "                continue\n",
        "            valid_token_list.append(mapping)\n",
        "        if len(valid_token_list) != len(word_labels):\n",
        "            print(index, len(word_labels), len(valid_token_list), sentence)\n",
        "        \n",
        "        # step 3: create token labels only for first word pieces of each tokenized word\n",
        "        labels = [labels_to_ids[label] for label in word_labels] \n",
        "        # code based on https://huggingface.co/transformers/custom_datasets.html#tok-ner\n",
        "        # create an empty array of -100 of length max_length\n",
        "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
        "        \n",
        "        # set only labels whose first offset position is 0 and the second is not 0\n",
        "        i = 0\n",
        "        if len(labels) != 0:\n",
        "            for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
        "                if mapping[0] == 0 and mapping[1] == 0:\n",
        "                    continue\n",
        "                tok = tokenizer.convert_ids_to_tokens(encoding['input_ids'][idx])\n",
        "            \n",
        "                # overwrite label\n",
        "                if i == len(labels):\n",
        "                    break\n",
        "                encoded_labels[idx] = labels[i]\n",
        "                i += 1\n",
        "                \n",
        "        # step 4: turn everything into PyTorch tensors\n",
        "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
        "        item['label'] = torch.as_tensor(encoded_labels)\n",
        "        \n",
        "        return item\n",
        "    def __len__(self):\n",
        "        return self.len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEgjXwJHy3px"
      },
      "outputs": [],
      "source": [
        "train_size = 0.8\n",
        "train_dataset = df.sample(frac=train_size,random_state=200)\n",
        "test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "\n",
        "training_set = ElectraDataset(train_dataset, tokenizer, MAX_LEN)\n",
        "testing_set = ElectraDataset(test_dataset, tokenizer, MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Cv53qH97a8P"
      },
      "outputs": [],
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 4\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 4\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knuKPJOiHbiZ",
        "outputId": "48c65dbc-9102-4907-90c5-4f2cf0382f7a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.9279, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "inputs = training_set[2]\n",
        "input_ids = inputs[\"input_ids\"].unsqueeze(0)\n",
        "attention_mask = inputs[\"attention_mask\"].unsqueeze(0)\n",
        "labels = inputs[\"label\"].unsqueeze(0)\n",
        "\n",
        "input_ids = input_ids.to(device)\n",
        "attention_mask = attention_mask.to(device)\n",
        "labels = labels.to(device)\n",
        "\n",
        "outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "initial_loss = outputs[0]\n",
        "initial_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTG-OFbTy3p_",
        "outputId": "55b9caa8-79ac-485c-f355-1ca6e01ed138"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 256, 152])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "tr_logits = outputs[1]\n",
        "tr_logits.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ezwpwVzvy3qB"
      },
      "outputs": [],
      "source": [
        "!pip install tensorboardX\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "writer = SummaryWriter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nfu7m9Zy3qC"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**5. train, validation 함수를 불러오는 섹션이다.**"
      ],
      "metadata": {
        "id": "8JPHdIBi4tp5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "train을 위한 함수이다."
      ],
      "metadata": {
        "id": "FAWjl3V54xQJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7KR_p-ky3qE"
      },
      "outputs": [],
      "source": [
        "# Defining the training function on the 80% of the dataset for tuning the bert model\n",
        "def train(epoch):\n",
        "    tr_loss, tr_accuracy = 0, 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    tr_preds, tr_labels = [], []\n",
        "    # put model in training mode\n",
        "    model.train()\n",
        "    \n",
        "    for idx, batch in enumerate(training_loader):\n",
        "        ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "        mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "        labels = batch['label'].to(device, dtype = torch.long)\n",
        "\n",
        "        output = model(ids, attention_mask=mask, labels=labels)\n",
        "        loss = output[0]\n",
        "        tr_logits = output[1]\n",
        "        tr_loss += loss.item()\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples += labels.size(0)\n",
        "        \n",
        "        if idx % 100==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            print(f\"Training loss per 100 training steps: {loss_step}\")\n",
        "           \n",
        "        # compute training accuracy\n",
        "        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
        "        active_logits = tr_logits.view(-1, model.config.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "        \n",
        "        # only compute accuracy at active labels\n",
        "        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
        "        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n",
        "        \n",
        "        labels = torch.masked_select(flattened_targets, active_accuracy)\n",
        "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "        \n",
        "        tr_labels.extend(labels)\n",
        "        tr_preds.extend(predictions)\n",
        "\n",
        "        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
        "        tr_accuracy += tmp_tr_accuracy\n",
        "    \n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
        "        )\n",
        "        \n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = tr_loss / nb_tr_steps\n",
        "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
        "    print(f\"Training loss epoch: {epoch_loss}\")\n",
        "    print(f\"Training accuracy epoch: {tr_accuracy}\")\n",
        "    writer.add_scalar('Train/Loss', epoch_loss, epoch)\n",
        "    writer.add_scalar('Train/Accuracy', tr_accuracy, epoch)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "validation을 위한 함수이다."
      ],
      "metadata": {
        "id": "BCKepAG841Qv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTx-ZhcSy3qJ"
      },
      "outputs": [],
      "source": [
        "def valid(epoch):\n",
        "    # put model in evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_examples, nb_eval_steps = 0, 0\n",
        "    eval_preds, eval_labels = [], []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(testing_loader):\n",
        "            \n",
        "            ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "            labels = batch['label'].to(device, dtype = torch.long)\n",
        "            \n",
        "            output = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
        "            loss = output[0]\n",
        "            eval_logits = output[1]\n",
        "            \n",
        "            eval_loss += loss.item()\n",
        "\n",
        "            nb_eval_steps += 1\n",
        "            nb_eval_examples += labels.size(0)\n",
        "        \n",
        "            if idx % 100==0:\n",
        "                loss_step = eval_loss/nb_eval_steps\n",
        "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
        "              \n",
        "            # compute evaluation accuracy\n",
        "            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = eval_logits.view(-1, model.config.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "            \n",
        "            # only compute accuracy at active labels\n",
        "            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
        "        \n",
        "            labels = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "            \n",
        "            eval_labels.extend(labels)\n",
        "            eval_preds.extend(predictions)\n",
        "            \n",
        "            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "    labels = [ids_to_labels[id.item()] for id in eval_labels]\n",
        "    predictions = [ids_to_labels[id.item()] for id in eval_preds]\n",
        "    \n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
        "    print(f\"Validation Loss: {eval_loss}\")\n",
        "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
        "    writer.add_scalar('Validation/Loss', eval_loss, epoch)\n",
        "    writer.add_scalar('Validation/Accuracy', eval_accuracy, epoch)\n",
        "\n",
        "    return labels, predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**6. save 함수를 불러오는 섹션이다.**\n",
        "> ***directory/model/tokenizer 이름 변경은 필수!***"
      ],
      "metadata": {
        "id": "S_4B-3dS45yn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54bGkXaCy3qP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "def save(epoch):\n",
        "    directory = \"/home/work/lesik/model/FIXED_EPOCH_\"+str(epoch)\n",
        "\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "    model.save_pretrained(directory)\n",
        "\n",
        "    torch.save(model.state_dict(), directory+\"/model.pt\")\n",
        "    directory = \"/home/work/lesik/tokenizer/FIXED_EPOCH_\" + str(epoch)\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "    # save vocabulary of the tokenizer\n",
        "    tokenizer.save_vocabulary(directory)\n",
        "    tokenizer.save_pretrained(directory)\n",
        "    # save the model weights and its configuration file\n",
        "    print('All files saved')\n",
        "    print('This tutorial is completed')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**7. 학습을 실행하는 코드이다.**\n",
        "> ***학습 과정에서 끊겼을 경우, prev_epoch 변경 필수! 그 외는 0으로 실행!***"
      ],
      "metadata": {
        "id": "abKjYSNQ5fmO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   prev_epoch는 학습을 시작하는 지점을 뜻하는 epoch이다.\n",
        "*   학습하다 끊겼을 경우, 저장 단위의 배수를 계산하여 마지막으로 저장된 epoch로 변경 해주면 된다. \\\n",
        "또한, 토큰화에서도 목표하고자 하는epoch를 저장된 epoch만큼 빼서 변경해줘야 한다.\n",
        "> ex.)epoch가 50에서 중단 되었을 경우, \\\n",
        "epoch 48까지 저장되었기 때문에 prev_epoch는 48로 시작. 토큰화의 epoch는 24로 변경. \\\n",
        "단, 51에서 중단되었을 경우, \\\n",
        "epoch 51이 저장되었다면, prev_epoch는 51부터 시작. 토큰화의 epoch는 21로 변경. \\\n",
        "epoch 51이 저장되지 않았다면,prev_epoch는 48부터 시작. 토큰화의 epoch는 24로 변경.\n",
        "*   epoch는 현재 3의 배수로 저장되고 있으며, 변경이 가능하다."
      ],
      "metadata": {
        "id": "zoH1Uw-M5nMw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJbOtD9fYnN1",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "prev_epoch = 0                   #학습을 시작하는 epoch\n",
        "for epoch in range(prev_epoch + 1, prev_epoch + 1 + EPOCHS):\n",
        "    print(f\"epoch: {epoch}\")\n",
        "    train(epoch)\n",
        "    valid(epoch)\n",
        "    if epoch != 0 and epoch % 3 == 0:     #현재 3의 배수로 저장되고 있으며 변경 가능 (3을 변경해주면 됩니다.)\n",
        "        save(epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmlstB95y3qW"
      },
      "outputs": [],
      "source": [
        "writer.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "PyTorch 1.13 (NGC 22.05/Python 3.8 Conda) on Backend.AI",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}